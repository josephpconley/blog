---
layout: post
type: post
tags: [comcast, chariot, ai, ml]
title: The O'Reilly AI Conference in NY
subtitle: asfasfda
published: false

---

I recently had the pleasure of attending the nascent O'Reilly AI Conference in Midtown Manhattan.  The event largely focused on the technical progress being made with deep learning techniques, reinforcement learning, and cognitive systems that focus on augmenting human intelligence.  These advancements have already had a significant impact in all walks of life, including autonomous driving, healthcare, and knowledge work to name just a few.  But my general impression from the conference was that while we can make significant economic gains in several specific domains (i.e. narrow AI applications), we're still a ways off from developing fully-thinking, superintelligent beings (i.e. strong AI).

The talks I enjoyed the most on Day One focused on building intelligent systems that worked as "thought partners" with humans.  Dave Ferrucci, the creator of Watson and head of [Emotional Intelligence], used this exact language to explain his company's approach to building intelligent systems.  He's working on building a foundation of knowledge for systems by having bots dialogue with a human counterpart to learn.  In this way, an intelligent system could learn much like a child does.

Another talk that I really enjoyed was "" by Hammond of [Narrative Science](narrativescience.com).  He talked about intelligent systems as storytellers.  Instead of focusing on fancy visualization for a data table in Excel, a system should be able to parse the table, do simple statistical analysis on the data, and use natural language generation to tell you about the data and what's interesting and important.  I love the efficiency in that!  We as developers spend so much time massaging, transforming, and visualizing data when ultimately we just need to answer a few simple questions.  Advances like Hammond's hold the promise of freeing us up to solve more interesting problems.

Systems like these really force you to think about the future of work, specifically knowledge work.  McKinsey gave an interesting talk on this very subject.  She mentioned that very specific classes of jobs will probably be automated by AI soon, like collecting and organizing data (e.g. adminstrative/knowledge work) and predictable physical work (e.g. driving a truck cross-country).  But wages aren't necessarily a good indicator of what can/can't be automated.  She mentioned landscaping as a low wage job that would be tough to automate, while high-wage lawyers are in greater danger.

I think this is an important thought exercise for any worker to go through.  Personally, I've been holding on to the belief that my job of software engineer was *very unlikely* to be replaced by a machine.  While I still maintain that belief, I can certainly envision a future where there's less explicit code being written and more reliance on probabilistic models.  However, someone's gotta maintain them right? (I hope).
 
Day Two had some interesting talks on reinforcement learning, especially in the keynotes.  [] at [] talked about their development of autonomous driving systems, and it was neat to see the iterations they went through to get a usable system.  Their initial effort resulted in a very defensive autonomous driver who would have strict requirements on when to change lanes or proceed at a 4-way stop intersection.  However, this initial approach was unusable.  When driving on a crowded California highway, you can't expect to get a nice safe cushion for changing a lane.
And other cars at a 4-way stop will inevitably inch forward while technically "waiting" for their turn, confusing the AI.  So the system developed a better strategy (organically) that merged defensive driving with a more collaborative approach.

Another neat example was Libratus, a heads-up, no-limit Texas Hold 'Em bot, who (again organically) developed a three-pronged strategy to playing poker.  It started with a base of ???.  Then during the later stage of the hand, it would employ an endgame solver to help analyze all possible permutations of play.  Finally, it would use a self-improve to analyze *it's own* historical play to find it's own weaknesses and improve on them.  As in other similar games, Libratus beat the world's best handily.  Though it might seem trivial, systems like Libratus could provide insight into other applications where imperfect information with one or more agents is relevant.

Finally, the keynote given by Peter Norvig, one of the fathers of AI, stressed how AI could revolutionize how we develop software.  His overall sentiment was getting to the point where engineers were more like teachers than plumbers, instructing machines how to model certain processes.  Software engineers are, at present, micromanagers, having to write every single instruction for the machine to handle.  It's refreshing to picture a world where coders could effectively build systems with more higher-level thinking but still have the confidence that the instructions will be interpreted and implemented without loss of meaning.

I really got a lot out of my time at the conference.  While the field of AI has gone through trends of popularity and decline in the past, it's hard to ignore the current wealth of applications and 

